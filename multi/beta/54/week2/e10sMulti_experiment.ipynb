{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# E10s/Multi testing for Beta 54\n",
    "\n",
    "\n",
    "## **NOTE: At the moment, Mac and Linux sample sizes are quite small as the cohorts still populate. Results for these OS's should not be used.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hack to render links in code output.\n",
    "from IPython.display import Markdown, display\n",
    "def print_with_markdown(md_text):\n",
    "    \"\"\" Print Markdown text so that it renders correctly in the cell output. \"\"\"\n",
    "    display(Markdown(md_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This covers data from the following range: 20170507_20170514"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RANGE = \"v20170507_20170514\"\n",
    "print_with_markdown(\"This covers data from the following range: {}\".format(RANGE.strip('v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:878: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to parse whitelist: /mnt/anaconda2/lib/python2.7/site-packages/moztelemetry/histogram-whitelists.json.\n",
      "Assuming all histograms are acceptable.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import ujson as json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from colour import Color\n",
    "import math\n",
    "import plotly.plotly as py\n",
    "import IPython\n",
    "import pyspark.sql.functions as fun\n",
    "import pyspark.sql.types as st\n",
    "from pyspark.sql import Row\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from moztelemetry.spark import get_pings, get_one_ping_per_client, get_pings_properties\n",
    "from montecarlino import grouped_permutation_test\n",
    "\n",
    "IPython.core.pylabtools.figsize(16, 7)\n",
    "sns.set_style('whitegrid')\n",
    "sc.setLogLevel('INFO')\n",
    "\n",
    "from operator import add\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "def get_active_addon_info(addons_str):\n",
    "    \"\"\" Return a list of currently enabled add-ons in the form (GUID, name, version, isSystem). \"\"\"\n",
    "    addons = json.loads(addons_str)\n",
    "    addons = addons.get(\"activeAddons\", {})\n",
    "    if not addons:\n",
    "        return []\n",
    "    return [(guid, meta.get(\"name\"), meta.get(\"isSystem\"), meta.get('isWebExtension'), meta.get('version')) for guid, meta in addons.iteritems()]\n",
    "\n",
    "def get_top_addons(df, cohort_filter, n_top=100):\n",
    "    cohort_num, cohort_table = dataset_installed_addons(\n",
    "        df.filter(cohort_filter),\n",
    "        n_top=n_top)\n",
    "    print(\"There were {:,} distinct add-ons installed across the '{}' cohort.\"\\\n",
    "          .format(cohort_num, cohort_filter))\n",
    "\n",
    "    cohort_table[\"n_installs\"] = cohort_table[\"n_installs\"]\n",
    "    cohort_table[\"pct_installed\"] = cohort_table[\"pct_installed\"]\n",
    "    return cohort_table\n",
    "\n",
    "\n",
    "def dataset_installed_addons(data, n_top=100):\n",
    "    \"\"\" Extract add-on info from a subset of the main dataset, and generate a table of top add-ons\n",
    "        with installation counts.\n",
    "        \n",
    "        Returns a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    data_addons = data.select(\"addons\").rdd.map(lambda row: row[\"addons\"])\n",
    "    data_addons.cache()\n",
    "    n_in_data = data_addons.count()\n",
    "    \n",
    "    ##  Get counts by add-on ID/name/isSystem value.\n",
    "    addon_counts = data_addons.flatMap(get_active_addon_info)\\\n",
    "        .map(lambda a: (a, 1))\\\n",
    "        .reduceByKey(add)\\\n",
    "        .map(lambda ((guid, name, sys, we, version), n): (guid, (name, sys, version, n)))\n",
    "    \n",
    "    ## Summarize using the most common name and isSystem value.\n",
    "    top_vals = addon_counts.reduceByKey(lambda a, b: a if a[-1] > b[-1] else b)\\\n",
    "        .map(lambda (guid, (name, sys, we, version, n)): (guid, (name, sys, version)))\n",
    "    n_installs = addon_counts.mapValues(lambda (name, sys, version, we, n): n)\\\n",
    "        .reduceByKey(add)\n",
    "    addon_info = top_vals.join(n_installs)\\\n",
    "        .map(lambda (guid, ((name, sys, we, version), n)): {\n",
    "                \"guid\": guid,\n",
    "                \"name\": name,\n",
    "                \"is_system\": sys,\n",
    "                \"is_webextension\": we,\n",
    "                \"version\":version,\n",
    "                \"n_installs\": n,\n",
    "                \"pct_installed\": n / n_in_data * 100\n",
    "            })\\\n",
    "        .sortBy(lambda info: info[\"n_installs\"], ascending=False)\n",
    "    \n",
    "    addon_info_coll = addon_info.collect() if not n_top else addon_info.take(n_top)\n",
    "    addon_info_table = pd.DataFrame(addon_info_coll)\n",
    "    addon_info_table = addon_info_table[[\"guid\", \"name\", \"version\",\"is_system\", \"is_webextension\", \"n_installs\", \"pct_installed\"]]\n",
    "    ## Number rows from 1.\n",
    "    addon_info_table.index += 1\n",
    "    n_addons = addon_info.count()\n",
    "    data_addons.unpersist()\n",
    "    return (n_addons, addon_info_table)\n",
    "\n",
    "def chi2_distance(xs, ys, eps = 1e-10, normalize = True):\n",
    "    \"\"\" The comparison metric for histograms. \"\"\"\n",
    "    histA = xs.sum(axis=0)\n",
    "    histB = ys.sum(axis=0)\n",
    "    \n",
    "    if normalize:\n",
    "        histA = histA/histA.sum()\n",
    "        histB = histB/histB.sum()\n",
    "    \n",
    "    d = 0.5 * np.sum([((a - b) ** 2) / (a + b + eps)\n",
    "        for (a, b) in zip(histA, histB)])\n",
    "\n",
    "    return d\n",
    "\n",
    "def median_diff(xs, ys):\n",
    "    return np.median(xs) - np.median(ys)\n",
    "\n",
    "def median(lst, n):\n",
    "    # take average of middle two numbers if even\n",
    "    cut = int(np.ceil(n/2))\n",
    "    return (lst[cut-1] + lst[cut]) / 2 if n % 2 == 0 else lst[cut-1]\n",
    "\n",
    "def make_group_histogram(group_data):\n",
    "    \"\"\" Combine separate client histograms into a single group histogram, normalizing bin counts\n",
    "        to relative frequencies.       \n",
    "    \"\"\"\n",
    "    N = len(group_data)\n",
    "    def median(lst, n):\n",
    "        # take average of middle two numbers if even\n",
    "        cut = int(np.ceil(n/2))\n",
    "        return (lst[cut-1] + lst[cut]) / 2 if n % 2 == 0 else lst[cut-1]\n",
    "            \n",
    "    ## Check for histograms with 0 counts.\n",
    "    client_totals = group_data.map(lambda x: x.sum())\n",
    "    group_data = group_data[client_totals > 0]\n",
    "    raw_counts = group_data.sum()\n",
    "    \n",
    "    # compute median\n",
    "    n = raw_counts.sum()\n",
    "    acc = -0.001\n",
    "    curr_dec = 0\n",
    "    deciles = []\n",
    "    dec = 0\n",
    "    for i, j in raw_counts.iteritems():\n",
    "        acc += j\n",
    "        while acc >= curr_dec:\n",
    "            deciles.append(i)\n",
    "            dec+=.1\n",
    "            curr_dec = n*dec\n",
    "                \n",
    "    ## Convert frequency counts to relative frequency for each client histogram.\n",
    "    group_data = group_data.map(lambda x: x/x.sum())\n",
    "    ## Merge the group's client histograms by adding up the frequencies over all clients\n",
    "    ## in the group, separately for each bin.\n",
    "    group_data = group_data.sum()\n",
    "    ## Convert the merged bin frequencies to relative percentages.\n",
    "    group_data= 100 * group_data / group_data.sum()\n",
    "    return group_data, deciles, N\n",
    "    \n",
    "\n",
    "def compare_histogram(histogram, webext_multi_1, webext_multi_4, multi_1=None, multi_4=None,\n",
    "                      include_diff=False, include_diff_in_diff=False, did_separate_plot=False):\n",
    "    \"\"\" Compare an e10s histogram to a non-e10s one, and graph the results.\n",
    "        \n",
    "        Plots the two histograms overlaid on the same graph, and prints a p-value\n",
    "        for testing whether they are different. If 'include_diff' is True, also\n",
    "        draw a plot of the frequency differences for each bin.\n",
    "        \n",
    "        If 'include_diff_in_diff' is True and data is supplied, include a plot of\n",
    "        differences between addon cohort differences and non-addon cohort differences.\n",
    "    \"\"\"\n",
    "    multi_1_total, dec1, n1 = make_group_histogram(webext_multi_1)\n",
    "    multi_4_total, dec4, n4 = make_group_histogram(webext_multi_4)\n",
    "    multi_1_total_std, dec_std1, n1_std = make_group_histogram(multi_1)\n",
    "    multi_4_total_std, dec_std4, n4_std = make_group_histogram(multi_4)\n",
    "    \n",
    "    ret = {\n",
    "            \"dec_1\": dec1,\n",
    "            \"dec_4\": dec4,\n",
    "            \"n_1\": n1,\n",
    "            \"n_4\": n4,\n",
    "            'dec_1_std': dec_std1,\n",
    "            'dec_4_std': dec_std4,\n",
    "            'n_1_std': n1_std,\n",
    "            'n_4_std': n4_std\n",
    "          }\n",
    "    \n",
    "    if include_diff:\n",
    "        if include_diff_in_diff and did_separate_plot:\n",
    "            fig, (ax, diff_ax, diff_diff_ax) = plt.subplots(3, sharex=True, figsize=(16,10), \n",
    "                                                            gridspec_kw={\"height_ratios\": [2,2,1]})\n",
    "        else:\n",
    "            fig, (ax, diff_ax) = plt.subplots(2, sharex=True)\n",
    "    else:\n",
    "        for comparison in [[multi_1_total, multi_4_total, webext_multi_1, webext_multi_4, n1, 'With WebExtensions'], \n",
    "                           [multi_1_total_std, multi_4_total_std, multi_1, multi_4, n4, \"Without WebExtensions\"]]:\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "            fig.subplots_adjust(hspace=0.3)\n",
    "            ax2 = ax.twinx()\n",
    "            width = 0.4\n",
    "            ylim = max(multi_1_total.max(), multi_4_total.max())\n",
    "            m1, m4, o1, o4, n, hist_desc = comparison\n",
    "            m1.plot(kind=\"bar\", alpha=0.5, color=\"green\", label=\"Multi 1\", ax=ax, width=width,\n",
    "                        position=1, ylim=(0, ylim + 1))\n",
    "            m4.plot(kind=\"bar\", alpha=0.5, color=\"blue\", label=\"Multi 4\", ax=ax2, width=width,\n",
    "                        position=0, grid=False, ylim=ax.get_ylim())\n",
    "\n",
    "            ## Combine legend info from both Axes.\n",
    "            ax_h, ax_l = ax.get_legend_handles_labels()\n",
    "            ax2_h, ax2_l = ax2.get_legend_handles_labels()\n",
    "            ax.legend(ax_h + ax2_h, ax_l + ax2_l, loc = 0)\n",
    "\n",
    "            ax.xaxis.grid(False)\n",
    "            ax.set_ylabel(\"Frequency %\")\n",
    "             # Only display at most 100 tick labels on the x axis.\n",
    "            xticklabs = plt.gca().get_xticklabels()\n",
    "            max_x_ticks = 100\n",
    "            if len(xticklabs) > max_x_ticks:\n",
    "                step_size = math.ceil(float(len(xticklabs)) / max_x_ticks)\n",
    "                for i, tl in enumerate(xticklabs):\n",
    "                    if i % step_size != 0:\n",
    "                        tl.set_visible(False)\n",
    "            ## Compute a p-value for the chi-square distance between the groups' combined histograms.\n",
    "            pvalue = grouped_permutation_test(chi2_distance, [o1, o4], num_samples=500)\n",
    "            if \"pvalue_14\" not in ret:\n",
    "                ret[\"pvalue_14\"] = pvalue\n",
    "            else:\n",
    "                ret[\"pvalue_14_std\"] = pvalue\n",
    "            print_with_markdown(\"\"\"---\\n#### {} {}\\nProbability the two histograms differ by chance is\n",
    "            <span style=\"color:{}\"><b>{:.3f}</b>.</span>\n",
    "            \"\"\".format(histogram, hist_desc ,\"red\" if pvalue <= .05 else \"green\", pvalue))\n",
    "            \n",
    "            \n",
    "            xticks, xticklabels = plt.xticks()\n",
    "            # shift half a step to the left\n",
    "            # x0 - (x1 - x0) / 2 = (3 * x0 - x1) / 2\n",
    "            xmin = (3*xticks[0] - xticks[1])/2.\n",
    "            # shaft half a step to the right\n",
    "            xmax = (3*xticks[-1] - xticks[-2])/2.\n",
    "            plt.xlim(xmin, xmax)\n",
    "            plt.xticks(xticks)\n",
    "            plt.show()\n",
    "            plt.show()\n",
    "        \n",
    "\n",
    "#     if include_diff:\n",
    "#         ## Add a second barplot of the difference in frequency for each bucket.\n",
    "#         #diff_ax = fig.add_subplot(2, 1, 2)\n",
    "#         enDiff = multi_1_total - multi_4_total\n",
    "        \n",
    "#         has_diff_in_diff_data = (multi_1 is not None and len(multi_1) > 0 and\n",
    "#                                  multi_4 is not None and len(multi_4) > 0)\n",
    "#         if include_diff_in_diff and has_diff_in_diff_data:\n",
    "#             ## Add bin differences for between e10s/non-e10s for the no-addons cohorts.\n",
    "#             ## The assumption is that the difference between addons cohorts would look the same\n",
    "#             ## if there is no additional effect of having addons.\n",
    "#             multi_1_total_std, dec_std1, n1_std = make_group_histogram(multi_1)\n",
    "#             multi_4_total_std, dec_std4, n4_std = make_group_histogram(multi_4)\n",
    "#             enDiff_std = multi_1_total_std - multi_4_total_std\n",
    "#             ylims = (min(enDiff.min(), enDiff_std.min()) - 0.5, max(enDiff.max(), enDiff_std.max()) + 0.5)\n",
    "#             diff_ax2 = diff_ax.twinx()\n",
    "            \n",
    "#             enDiff.plot(kind=\"bar\", alpha=0.5, color=\"navy\", label=\"WebExtensions\", ax=diff_ax, width=width,\n",
    "#                         position=1, ylim=ylims)\n",
    "#             enDiff_std.plot(kind=\"bar\", alpha=0.5, color=\"gray\", label=\"no WebExtensions\", ax=diff_ax2, width=width,\n",
    "#                         position=0, grid=False, ylim=diff_ax.get_ylim())\n",
    "\n",
    "#             ## Combine legend info from both Axes.\n",
    "#             diff_ax_h, diff_ax_l = diff_ax.get_legend_handles_labels()\n",
    "#             diff_ax2_h, diff_ax2_l = diff_ax2.get_legend_handles_labels()\n",
    "#             leg_h = diff_ax_h + diff_ax2_h\n",
    "#             leg_l = diff_ax_l + diff_ax2_l\n",
    "            \n",
    "#             if did_separate_plot:\n",
    "#                 enDiffDiff = enDiff - enDiff_std\n",
    "#                 enDiffDiff.plot(kind=\"bar\", alpha=0.5, color=\"maroon\", ax=diff_diff_ax, ylim=diff_ax.get_ylim())\n",
    "#                 diff_diff_ax.xaxis.grid(False)\n",
    "#                 diff_diff_ax.set_ylabel(\"Diff in freq %\")\n",
    "#                 diff_diff_ax.set_title(\"Diff between multi 1/4 with webextensions and multi 1/4 diff without webextensions\" +\n",
    "#                                       \" (with webextensions higher when > 0)\")\n",
    "            \n",
    "#         else:\n",
    "#             if include_diff_in_diff:\n",
    "#                 ## We wanted to do the additional comparison, but there wasn't enough data.\n",
    "#                 print(\"\\nNo diff-in-diff comparison: one of the standard cohorts has no non-missing observations.\")\n",
    "#             enDiff.plot(kind=\"bar\", alpha=0.5, color=\"navy\", label=\"WebExtensions\", ax=diff_ax)\n",
    "#             leg_h, leg_l = diff_ax.get_legend_handles_labels()\n",
    "        \n",
    "#         plt.title(\"multi1/multi4 difference (more multi1 in bucket when > 0)\")\n",
    "#         diff_ax.xaxis.grid(False)\n",
    "#         diff_ax.set_ylabel(\"Diff in frequency %\")\n",
    "#         diff_ax.legend(leg_h, leg_l, loc = 0)\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "   \n",
    "        \n",
    "           \n",
    "#     if include_diff_in_diff:\n",
    "#         pvalue_std = grouped_permutation_test(chi2_distance, [multi_1, multi_4], num_samples=1000)\n",
    "#         print(\"The probability that the distributions for {} (without webextensions)\\nare differing by chance is {:.3f}.\"\\\n",
    "#           .format(histogram, pvalue_std))\n",
    "        \n",
    "    return ret\n",
    "\n",
    "def normalize_uptime_hour(frame):\n",
    "    \"\"\" Convert metrics to rates per hour of uptime. \"\"\"\n",
    "    frame = frame[frame[\"payload/simpleMeasurements/totalTime\"] > 60]\n",
    "    removed_os = False\n",
    "    os = None\n",
    "    try:\n",
    "        os = frame[\"system/os/name\"]\n",
    "        frame = frame.drop(\"system/os/name\", axis=1)\n",
    "        removed_os = True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    frame = 60 * 60 * frame.apply(lambda x: x / frame[\"payload/simpleMeasurements/totalTime\"]) # Metric per hour\n",
    "    frame.drop('payload/simpleMeasurements/totalTime', axis=1, inplace=True)\n",
    "    if removed_os:\n",
    "        frame[\"system/os/name\"] = os\n",
    "    return frame\n",
    "    \n",
    "def compare_e10s_count_histograms(pings, cohort_sizes = {}, *histogram_names, **kwargs):\n",
    "    \"\"\" Read multiple count histograms from a collection of pings, and compare e10s/non-e10s for each.\n",
    "    \n",
    "        Treats count histograms as scalars for comparison purposes, without distinguishing between\n",
    "        parent and child processes. Expects a dict containing overall cohort sizes\n",
    "        for computing sample size proportions.\n",
    "    \"\"\"\n",
    "    properties = histogram_names + (\"payload/simpleMeasurements/totalTime\", \"cohort\", \"system/os/name\")\n",
    "    frame = pd.DataFrame(get_pings_properties(pings, properties).collect())\n",
    "    \n",
    "    ret = defaultdict(dict)\n",
    "    for os in (\"Windows_NT\", \"Darwin\"):\n",
    "        print_with_markdown(\"---\")\n",
    "        print_with_markdown(\"# {}\".format(os))\n",
    "        frame_os = frame[frame['system/os/name'] == os]\n",
    "        # multi with webextensions\n",
    "        we_multi1 = normalize_uptime_hour(\n",
    "                frame_os[frame_os.cohort==\"webextensions-multiBucket1\"].drop(\"cohort\",axis=1))\n",
    "        we_multi4 = normalize_uptime_hour(\n",
    "            frame_os[frame_os.cohort==\"webextensions-multiBucket4\"].drop(\"cohort\", axis=1))\n",
    "\n",
    "    \n",
    "        include_diff_in_diff = kwargs.get(\"include_diff_in_diff\", True)\n",
    "        if include_diff_in_diff:\n",
    "            multi1 = normalize_uptime_hour(frame_os[frame_os.cohort==\"multiBucket1\"].drop(\"cohort\", axis=1))\n",
    "            multi4 = normalize_uptime_hour(frame_os[frame_os.cohort==\"multiBucket4\"].drop(\"cohort\", axis=1))     \n",
    "\n",
    "        for histogram in histogram_names:\n",
    "            if histogram not in multi1.columns:\n",
    "                continue\n",
    "\n",
    "            ## Remove the property path from the histogram name for display purposes.\n",
    "            hist_name = hist_base_name(histogram)\n",
    "            if type(hist_name) == list:\n",
    "                ## Key was given for keyed histogram.\n",
    "                hist_str = \"{}/{}\".format(link_to_histogram(hist_name[0]), hist_name[1])\n",
    "                hist_name = hist_name[0]\n",
    "            else:\n",
    "                hist_str = hist_name\n",
    "            ## Print a header for the block of graphs, including a link to the histogram definition.\n",
    "            print_with_markdown(\"Comparison for count histogram {}:\".format(hist_str))\n",
    "\n",
    "            we_multi1_hist = we_multi1[[histogram, 'system/os/name']].dropna()\n",
    "            we_multi4_hist = we_multi4[[histogram, 'system/os/name']].dropna()\n",
    "\n",
    "            ## Print some information on sample sizes.\n",
    "            print(\"{} Multi 4 profiles (with webextenisons) have this histogram.\".format(\n",
    "                    sample_size_str(len(we_multi4_hist), cohort_sizes.get(WEBEXTENSION_MULTI_4))))\n",
    "            print(\"{} Multi 1 profiles (with webextensions) have this histogram.\".format(\n",
    "                    sample_size_str(len(we_multi1_hist), cohort_sizes.get(WEBEXTENSION_MULTI_1))))\n",
    "            ## If either group has no data, nothing more to do.\n",
    "            if len(we_multi4_hist) == 0 or len(we_multi1_hist) == 0:\n",
    "                continue\n",
    "\n",
    "            print(\"\")\n",
    "            hist_name = histogram.split('/')[-1]\n",
    "            ret[os][hist_name] =  compare_scalars(hist_name + \" per hour\", we_multi1_hist, we_multi4_hist,\n",
    "                            multi1[[histogram, 'system/os/name']].dropna() if include_diff_in_diff else None,\n",
    "                            multi4[[histogram, 'system/os/name']].dropna() if include_diff_in_diff else None)\n",
    "    return ret\n",
    " \n",
    "def compare_e10s_histograms(pings, cohort_sizes = {}, *histogram_names, **kwargs):\n",
    "    \"\"\" Read multiple histograms from a collection of pings, and compare multi1 /\n",
    "        multi4 + webextenson / no webextension   for each.\n",
    "    \n",
    "        Outputs separate comparisons for parent process, child processes, and merged histograms.\n",
    "        Expects a dict containing overall cohort sizes for computing sample\n",
    "        size proportions.\n",
    "    \"\"\"\n",
    "    ## Load histogram data from the ping set, separating parent & child processes for e10s.\n",
    "\n",
    "    has_hist = lambda f, samp, suff: f([bool(samp[i + suff]) for i in (\"multi_4\", \"multi_4\", \"webext_multi_1\", \"webext_multi_4\")])\n",
    "\n",
    "    frame = pd.DataFrame(get_pings_properties(pings, histogram_names + (\"cohort\", \"has_webextension\", \"system/os/name\"), \n",
    "                                              with_processes=True)\\\n",
    "        .collect())\n",
    "    \n",
    "    ret = defaultdict(dict)\n",
    "    for os in (\"Windows_NT\", \"Darwin\"):\n",
    "        print_with_markdown(\"# {}\".format(os))\n",
    "        frame_os = frame[frame['system/os/name'] == os]\n",
    "        # multi with no webextenions\n",
    "        multi1 = frame_os[frame_os.cohort==\"multiBucket1\"]\n",
    "        multi4 = frame_os[frame_os.cohort==\"multiBucket4\"]\n",
    "        # multi with webextensions\n",
    "        we_multi1 = frame_os[frame_os.cohort==\"webextensions-multiBucket1\"]\n",
    "        we_multi4 = frame_os[frame_os.cohort==\"webextensions-multiBucket4\"]\n",
    "\n",
    "        for histogram in histogram_names:\n",
    "            if histogram not in we_multi1.columns:\n",
    "                continue\n",
    "\n",
    "            ## Remove the property path from the histogram name for display purposes.\n",
    "            hist_name = hist_base_name(histogram)\n",
    "            if type(hist_name) == list:\n",
    "                ## Key was given for keyed histogram.\n",
    "                hist_str = \"{}/{}\".format(link_to_histogram(hist_name[0]), hist_name[1])\n",
    "                hist_name = hist_name[0]\n",
    "            else:\n",
    "                hist_str = hist_name\n",
    "            ## Print a header for the block of graphs, including a link to the histogram definition.\n",
    "            print_with_markdown(\"Comparison for {}:\".format(hist_str))\n",
    "\n",
    "            ## Compare main, parent and child histograms\n",
    "            addons_hist_data = {\n",
    "#                 \"multi_1_merged\": multi1[histogram],\n",
    "#                 \"multi_4_merged\": multi4[histogram],\n",
    "#                 \"webext_multi_1_merged\": we_multi1[histogram],\n",
    "#                 \"webext_multi_4_merged\": we_multi4[histogram],\n",
    "                \"multi_1_parent\": multi1[histogram + \"_parent\"],\n",
    "                \"multi_4_parent\": multi4[histogram + \"_parent\"],\n",
    "                \"webext_multi_1_parent\": we_multi1[histogram + \"_parent\"],\n",
    "                \"webext_multi_4_parent\": we_multi4[histogram + \"_parent\"],\n",
    "                \"multi_1_child\": multi1[histogram + \"_children\"],\n",
    "                \"multi_4_child\": multi4[histogram + \"_children\"],\n",
    "                \"webext_multi_1_child\": we_multi1[histogram + \"_children\"],\n",
    "                \"webext_multi_4_child\": we_multi4[histogram + \"_children\"],\n",
    "\n",
    "            }\n",
    "            for htype in addons_hist_data:\n",
    "                addons_hist_data[htype] = addons_hist_data[htype].dropna()\n",
    "\n",
    "            ## Print some information on sample sizes.\n",
    "            sample_sizes = { htype: len(hdata) for htype, hdata in addons_hist_data.iteritems() }\n",
    "#             print(\"{} multi_1 profiles have this histogram.\".format(\n",
    "#                     sample_size_str(sample_sizes[\"multi_1_merged\"], cohort_sizes.get(MULTI_1))))\n",
    "#             print(\"{} multi_4 profiles have this histogram.\".format(\n",
    "#                     sample_size_str(sample_sizes[\"multi_4_merged\"], cohort_sizes.get(MULTI_4))))\n",
    "#             print(\"{} webext_multi_1 profiles have this histogram.\".format(\n",
    "#                     sample_size_str(sample_sizes[\"webext_multi_1_merged\"], cohort_sizes.get(WEBEXTENSION_MULTI_1))))\n",
    "#             print(\"{} webext_multi_4 profiles have this histogram.\".format(\n",
    "#                     sample_size_str(sample_sizes[\"webext_multi_4_merged\"], cohort_sizes.get(WEBEXTENSION_MULTI_4))))\n",
    "\n",
    "#             ## If either group has no data, nothing more to do.\n",
    "#             if not has_hist(any, sample_sizes, \"_merged\"):\n",
    "#                 print \"No hists found\"\n",
    "#                 continue\n",
    "\n",
    "            print(\"{} multi_1 profiles on {} have the parent histogram.\".format(\n",
    "                    sample_size_str(sample_sizes[\"multi_1_parent\"], cohort_sizes.get(os + \"_\" + MULTI_1)), os))\n",
    "            print(\"{} multi_4 profiles on {} have the parent histogram.\".format(\n",
    "                    sample_size_str(sample_sizes[\"multi_4_parent\"], cohort_sizes.get(os + \"_\" + MULTI_4)), os))\n",
    "            print(\"{} webext_multi_1 profiles on {} have the parent histogram.\".format(\n",
    "                    sample_size_str(sample_sizes[\"webext_multi_1_parent\"], cohort_sizes.get(os + \"_\" + WEBEXTENSION_MULTI_1)), os))\n",
    "            print(\"{} webextx_multi_4 profiles on {} have the parent histogram.\".format(\n",
    "                    sample_size_str(sample_sizes[\"webext_multi_4_parent\"], cohort_sizes.get(os + \"_\" + WEBEXTENSION_MULTI_4)), os))\n",
    "\n",
    "            print(\"{} multi_1 profiles on {} have the child histogram.\".format(\n",
    "                    sample_size_str(sample_sizes[\"multi_1_child\"], cohort_sizes.get(os + \"_\" + MULTI_1)), os))\n",
    "            print(\"{} multi_4 profiles on {} have the child histogram.\".format(\n",
    "                    sample_size_str(sample_sizes[\"multi_4_child\"], cohort_sizes.get(os + \"_\" + MULTI_4)), os))\n",
    "            print(\"{} webext_multi_1 profiles on {} have the child histogram.\".format(\n",
    "                    sample_size_str(sample_sizes[\"webext_multi_1_child\"], cohort_sizes.get(os + \"_\" + WEBEXTENSION_MULTI_1)), os))\n",
    "            print(\"{} webextx_multi_4 profiles on {} have the child histogram.\".format(\n",
    "                    sample_size_str(sample_sizes[\"webext_multi_4_child\"], cohort_sizes.get(os + \"_\" + WEBEXTENSION_MULTI_4)), os))\n",
    "\n",
    "            has_parents = has_hist(all, sample_sizes, \"_parent\")\n",
    "            has_children = has_hist(all, sample_sizes, \"_child\")\n",
    "    \n",
    "            result = None\n",
    "\n",
    "#             ## Compare merged histograms, groups have either no parents or children\n",
    "#             if has_children and has_parents:\n",
    "#                 result = compare_histogram(hist_name + \" (merged)\", \n",
    "#                                   addons_hist_data[\"webext_multi_1_merged\"],\n",
    "#                                   addons_hist_data[\"webext_multi_4_merged\"],\n",
    "#                                   addons_hist_data[\"multi_1_merged\"],\n",
    "#                                   addons_hist_data[\"multi_4_merged\"],\n",
    "#                                   **kwargs)\n",
    "#                 ret[os][hist_name + \" (merged)\"] = result\n",
    "\n",
    "            if has_parents:\n",
    "                result = compare_histogram(hist_name + \" (parent)\",\n",
    "                                  addons_hist_data[\"webext_multi_1_parent\"],\n",
    "                                  addons_hist_data[\"webext_multi_4_parent\"],\n",
    "                                  addons_hist_data[\"multi_1_parent\"],\n",
    "                                  addons_hist_data[\"multi_4_parent\"],\n",
    "                                  **kwargs)\n",
    "                ret[os][hist_name + \" (parent)\"] = result\n",
    "\n",
    "            if has_children:\n",
    "                result = compare_histogram(hist_name + \" (children)\",\n",
    "                                  addons_hist_data[\"webext_multi_1_child\"],\n",
    "                                  addons_hist_data[\"webext_multi_4_child\"],\n",
    "                                  addons_hist_data[\"multi_1_child\"],\n",
    "                                  addons_hist_data[\"multi_4_child\"],\n",
    "                                  **kwargs)\n",
    "                ret[os][hist_name + \" (child)\"] = result\n",
    "    return ret\n",
    "\n",
    "def compare_scalars(metric, we_multi1, we_multi4, multi1=None, multi4=None, unit=\"units\"):\n",
    "    \"\"\" Prints info about the median difference between the groups, together with a p-value\n",
    "        for testing the difference.\n",
    "        \n",
    "        Optionally include a string indicating the units the metric is measured in.\n",
    "        If data is supplied, also print a comparison for non-addons cohorts.\n",
    "    \"\"\"\n",
    "    def fix_nested_series(series):\n",
    "        x = np.zeros(len(series.iloc[0]))\n",
    "        for i in series:\n",
    "            x += i\n",
    "        return x\n",
    "    \n",
    "    ret = defaultdict(dict)\n",
    "    col = we_multi1.columns[0]\n",
    "    histogram = metric\n",
    "    we_multi1 = we_multi1.dropna()[col]\n",
    "    we_multi4 = we_multi4.dropna()[col]\n",
    "    if len(we_multi1) == 0 or len(we_multi4) == 0:\n",
    "        print(\"Cannot run comparison: one of the groups has no non-missing observations.\")\n",
    "        return\n",
    "    print(\"Comparison for {}{} (with webextensions):\\n\".format(metric, \" ({})\".format(unit) if unit != \"units\" else \"\"))\n",
    "\n",
    "    if type(we_multi1.iloc[0]) == pd.Series:\n",
    "        we_multi1 = fix_nested_series(we_multi1)\n",
    "    if type(we_multi4.iloc[0]) == pd.Series:\n",
    "        we_multi4 = fix_nested_series(we_multi4)    \n",
    "\n",
    "    we1_n = len(we_multi1[we_multi1 > 0])\n",
    "    we4_n = len(we_multi4[we_multi4 > 0])\n",
    "\n",
    "    we1_dec = np.percentile(we_multi1, np.arange(0, 100, 10))\n",
    "    we4_dec = np.percentile(we_multi4, np.arange(0, 100, 10))\n",
    "\n",
    "\n",
    "    mdiff = median_diff(we_multi1, we_multi4)\n",
    "\n",
    "    print(\"- Median with 1 content process is {:.3g} {} {} median with 4 processes.\"\\\n",
    "         .format(\n",
    "            #abs(mdiff),\n",
    "            mdiff,\n",
    "            unit,\n",
    "            #\"higher than\" if mdiff >= 0 else \"lower than\"\n",
    "            \"different from\"))\n",
    "    print(\"- This is a relative difference of {:.1f}%.\".format(float(mdiff) / we4_dec[5] * 100))\n",
    "    print(\"- Multi 1 group median is {:.4g}, Multi 4 group median is {:.4g}.\".format(we1_dec[5], we4_dec[5]))\n",
    "\n",
    "    pvalue = grouped_permutation_test(median_diff, [we_multi1, we_multi4], num_samples=10000)\n",
    "    print_with_markdown(\"\"\"\\n (with webextensions) The probability the two histograms differ by chance is\n",
    "            <span style=\"color:{}\"><b>{:.3f}</b>.</span>\n",
    "            \"\"\".format(\"red\" if pvalue <= .05 else \"green\", pvalue))\n",
    "\n",
    "    mname = col.split(\"/\")[-1] \n",
    "    ret = {\n",
    "                    'dec_1': we1_dec,\n",
    "                    'dec_4': we4_dec,\n",
    "                    'n_1': we1_n,\n",
    "                    'n_4': we4_n,\n",
    "                    'pvalue_14': pvalue\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "    if multi1 is not None and multi4 is not None:\n",
    "        ## Include a comparison between non-addon cohorts.\n",
    "        multi1_s = multi1.dropna()[col]\n",
    "        multi4_s = multi4.dropna()[col]\n",
    "        if len(multi1_s) > 0 and len(multi4_s) > 0:\n",
    "            if type(multi1_s.iloc[0]) == pd.Series:\n",
    "                multi1_s = fix_nested_series(multi1_s)\n",
    "            if type(multi4_s.iloc[0]) == pd.Series:\n",
    "                multi4_s = fix_nested_series(multi4_s)   \n",
    "\n",
    "            m1_n = len(multi1_s[multi1_s > 0])\n",
    "            m4_n= len(multi4_s[multi4_s > 0])\n",
    "\n",
    "            m1_dec = np.percentile(multi1_s, np.arange(0, 100, 10))\n",
    "            m4_dec = np.percentile(multi4_s, np.arange(0, 100, 10))\n",
    "            \n",
    "            mdiff_std = median_diff(multi1_s, multi4_s)\n",
    "            print(\"\\nFor cohorts with no webextensions, median with 1 content process is {:.3g} {} ({:.1f}%) {} median with 4 processes\"\\\n",
    "                 .format(\n",
    "                    #abs(mdiff_std),\n",
    "                    mdiff_std,\n",
    "                    unit,\n",
    "                    float(mdiff_std) / m4_dec[5] * 100,\n",
    "                    #\"higher than\" if mdiff_std >= 0 else \"lower than\"\n",
    "                    \"different from\"))\n",
    "            print(\"- This is a relative difference of {:.1f}%.\"\\\n",
    "                  .format(float(mdiff_std) / m4_dec[5] * 100))\n",
    "            print(\"- Multi 1 group median is {:.4g}, Multi 4 group median is {:.4g}.\"\\\n",
    "                  .format(m1_dec[5], m4_dec[5]))\n",
    "\n",
    "            pvalue_std = grouped_permutation_test(median_diff, [multi1_s, multi4_s], num_samples=10000)\n",
    "        \n",
    "            \n",
    "            print_with_markdown(\"\"\"(without WebExtensions) The probability the two histograms differ by chance is\n",
    "            <span style=\"color:{}\"><b>{:.3f}</b>.</span>\n",
    "            \"\"\".format(\"red\" if pvalue_std <= .05 else \"green\", pvalue_std))\n",
    "\n",
    "            ret[\"dec_1_std\"] = m1_dec\n",
    "            ret[\"dec_4_std\"] = m4_dec\n",
    "            ret[\"n_1_std\"] = m1_n\n",
    "            ret[\"n_4_std\"] = m4_n\n",
    "            ret[\"pvalue_14_std\"] = pvalue_std\n",
    "            \n",
    "    return ret\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def link_to_histogram(hist_name):\n",
    "    \"\"\" Create a link to the histogram definition in Markdown. \"\"\"\n",
    "    return \"[{}](https://dxr.mozilla.org/mozilla-central/search?q={}+file%3AHistograms.json&redirect=true)\"\\\n",
    "            .format(hist_name, hist_name)\n",
    "\n",
    "def hist_base_name(path_to_histogram):\n",
    "    \"\"\" Remove any path components from histogram name.\n",
    "    \n",
    "        If histogram is specified as a path in the payload, with separator '/',\n",
    "        remove everything but the last component (the actual name).\n",
    "        However, if the histogram is keyed, and specified with a key, return\n",
    "        [histname, key].\n",
    "    \"\"\"\n",
    "    path_to_histogram = path_to_histogram.rsplit(\"/\")\n",
    "    if len(path_to_histogram) > 1 and path_to_histogram[-3] == \"keyedHistograms\":\n",
    "        ## There was a keyedHistogram name and key given.\n",
    "        return path_to_histogram[-2:]\n",
    "    return path_to_histogram[-1]\n",
    "\n",
    "\n",
    "def sample_size_str(sample_size, cohort_size=None):\n",
    "    \"\"\" Convert a sample size to a string representation, including a percentage if available. \"\"\"\n",
    "    if sample_size == 0:\n",
    "        return \"No\"\n",
    "    if cohort_size:\n",
    "        if sample_size == cohort_size:\n",
    "            return \"All\"\n",
    "        return \"{} ({:.1f}%)\".format(sample_size, float(sample_size) / cohort_size * 100)\n",
    "    return str(sample_size)\n",
    "\n",
    "def get_cohort_dist(dataset):\n",
    "    cohort_counts = dataset.groupby(\"e10sCohort\").count().collect()\n",
    "    dataset_count = sum(map(lambda r: r[\"count\"], cohort_counts))\n",
    "\n",
    "    def cohort_proportions(r):\n",
    "        prop = r[\"count\"] * 100.0 / dataset_count\n",
    "        return (r[\"e10sCohort\"], r[\"count\"], \"{:.2f}%\".format(prop))\n",
    "\n",
    "    print(\"\\nTotal number of clients: {:,}\".format(dataset_count))\n",
    "\n",
    "    d = pd.DataFrame(sorted(map(cohort_proportions, cohort_counts), key = lambda r: r[0]))\n",
    "    d.columns = ['cohort', 'count', 'pct']\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derived dataset is computed from profiles on Beta 54 who belong to the {webextensions-}multiBucket{1, 4} e10s Cohorts. It contains a single record (ping) per client, which is randomly selected from among the client's pings during the date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# regenerated data and loaded into telemetry-test-bucket\n",
    "dataset = sqlContext.read.parquet(\n",
    "    \"s3://telemetry-parquet/e10s_experiment_view/multi_webExtensions_beta54_cohorts/{}/\".format(RANGE))\n",
    "# dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records are in the overall dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2981099"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the cohorts, and how many clients do we have in each cohort?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of clients: 2,981,099\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>count</th>\n",
       "      <th>pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>addons-set50allmpc-control</td>\n",
       "      <td>15342</td>\n",
       "      <td>0.51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>addons-set50allmpc-multiBucket1</td>\n",
       "      <td>980</td>\n",
       "      <td>0.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>addons-set50allmpc-multiBucket2</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>addons-set50allmpc-multiBucket4</td>\n",
       "      <td>1031</td>\n",
       "      <td>0.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>addons-set50allmpc-multiBucket8</td>\n",
       "      <td>1019</td>\n",
       "      <td>0.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>addons-set50allmpc-test</td>\n",
       "      <td>92934</td>\n",
       "      <td>3.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>addons-set51alladdons-control</td>\n",
       "      <td>14</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>addons-set51alladdons-test</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>addons-set51set1-test</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>control</td>\n",
       "      <td>86816</td>\n",
       "      <td>2.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>disqualified</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>disqualified-control</td>\n",
       "      <td>112668</td>\n",
       "      <td>3.78%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>disqualified-multiBucket1</td>\n",
       "      <td>2531</td>\n",
       "      <td>0.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>disqualified-multiBucket2</td>\n",
       "      <td>2456</td>\n",
       "      <td>0.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>disqualified-multiBucket4</td>\n",
       "      <td>2442</td>\n",
       "      <td>0.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>disqualified-multiBucket8</td>\n",
       "      <td>2556</td>\n",
       "      <td>0.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>disqualified-test</td>\n",
       "      <td>982924</td>\n",
       "      <td>32.97%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>multiBucket1</td>\n",
       "      <td>674373</td>\n",
       "      <td>22.62%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>multiBucket2</td>\n",
       "      <td>37520</td>\n",
       "      <td>1.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>multiBucket4</td>\n",
       "      <td>673671</td>\n",
       "      <td>22.60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>multiBucket8</td>\n",
       "      <td>37308</td>\n",
       "      <td>1.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>optedIn</td>\n",
       "      <td>2546</td>\n",
       "      <td>0.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>optedOut</td>\n",
       "      <td>15301</td>\n",
       "      <td>0.51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>temp-disqualified-ru</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>temp-qualified-devtools</td>\n",
       "      <td>147133</td>\n",
       "      <td>4.94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>test</td>\n",
       "      <td>4526</td>\n",
       "      <td>0.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>unknown</td>\n",
       "      <td>3380</td>\n",
       "      <td>0.11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>unsupportedChannel</td>\n",
       "      <td>56</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>webextensions-multiBucket1</td>\n",
       "      <td>40456</td>\n",
       "      <td>1.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>webextensions-multiBucket4</td>\n",
       "      <td>40072</td>\n",
       "      <td>1.34%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             cohort   count     pct\n",
       "0        addons-set50allmpc-control   15342   0.51%\n",
       "1   addons-set50allmpc-multiBucket1     980   0.03%\n",
       "2   addons-set50allmpc-multiBucket2    1021   0.03%\n",
       "3   addons-set50allmpc-multiBucket4    1031   0.03%\n",
       "4   addons-set50allmpc-multiBucket8    1019   0.03%\n",
       "5           addons-set50allmpc-test   92934   3.12%\n",
       "6     addons-set51alladdons-control      14   0.00%\n",
       "7        addons-set51alladdons-test       8   0.00%\n",
       "8             addons-set51set1-test       7   0.00%\n",
       "9                           control   86816   2.91%\n",
       "10                     disqualified       7   0.00%\n",
       "11             disqualified-control  112668   3.78%\n",
       "12        disqualified-multiBucket1    2531   0.08%\n",
       "13        disqualified-multiBucket2    2456   0.08%\n",
       "14        disqualified-multiBucket4    2442   0.08%\n",
       "15        disqualified-multiBucket8    2556   0.09%\n",
       "16                disqualified-test  982924  32.97%\n",
       "17                     multiBucket1  674373  22.62%\n",
       "18                     multiBucket2   37520   1.26%\n",
       "19                     multiBucket4  673671  22.60%\n",
       "20                     multiBucket8   37308   1.25%\n",
       "21                          optedIn    2546   0.09%\n",
       "22                         optedOut   15301   0.51%\n",
       "23             temp-disqualified-ru       1   0.00%\n",
       "24          temp-qualified-devtools  147133   4.94%\n",
       "25                             test    4526   0.15%\n",
       "26                          unknown    3380   0.11%\n",
       "27               unsupportedChannel      56   0.00%\n",
       "28       webextensions-multiBucket1   40456   1.36%\n",
       "29       webextensions-multiBucket4   40072   1.34%"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cohort_dist(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WEBEXTENSION_MULTI_1 = u'webextensions-multiBucket1'\n",
    "WEBEXTENSION_MULTI_4 = u'webextensions-multiBucket4'\n",
    "MULTI_1 = u'multiBucket1'\n",
    "MULTI_4 = u'multiBucket4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict to pings belonging to the Multi-WebExtensions experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "webext_exp_dataset = dataset.filter(\\\n",
    "\"e10sCohort in ('%s','%s', '%s', '%s')\" % (WEBEXTENSION_MULTI_1,\n",
    "                                           WEBEXTENSION_MULTI_4,\n",
    "                                           MULTI_1,\n",
    "                                           MULTI_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clients are left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1428572"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webext_exp_dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that the pings tagged into the cohorts satisfy the basic assumptions of the experiment, as this not guaranteed. Both webextensions-multiBucket{1, 4} cohorts should have only WebExtension add-ons, and both multiBucket{1, 4} cohorts should have no add-ons. All profiles should have e10s enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def e10s_status_check(settings, addons):\n",
    "    \"\"\" \n",
    "    Check that multiBucket cohorts have no (non-system) add-ons, \n",
    "    and that webextensions-multiBucket cohorts have only webextensions\n",
    "    (system add-ons are ok). All cohorts should have e10s on\n",
    "        \n",
    "    \"\"\"\n",
    "    e10sEnabled = bool(json.loads(settings).get(\"e10sEnabled\"))\n",
    "    aa = json.loads(addons).get(\"activeAddons\")\n",
    "    \n",
    "    # check if all addons (that are non-system) are webextensions\n",
    "    webext_status = [aa[i].get('isWebExtension', False) for i in aa \\\n",
    "                    if not aa[i].get('isSystem', False)] if aa else []\n",
    "    onlyWebExtension = all(webext_status) if len(webext_status) > 0 else False\n",
    "    \n",
    "    return Row(\n",
    "        e10s_enabled = e10sEnabled,\n",
    "        only_webextension = onlyWebExtension\n",
    "   )\n",
    "\n",
    "def bad_ping(cohort, settings, addons):\n",
    "    \"\"\" e10s should be enabled iff the profile is in the test cohort, and profiles should have active add-ons\n",
    "        if they are in the addons cohorts. \n",
    "    \"\"\"\n",
    "    check_data = e10s_status_check(settings, addons)\n",
    "    # must have e10s\n",
    "    is_bad = not check_data.e10s_enabled\n",
    "    if cohort.startswith(\"webextensions\"):\n",
    "        is_bad = is_bad or not check_data.only_webextension\n",
    "    return is_bad\n",
    "\n",
    "## Add a Column to the DF with the outcome of the check.\n",
    "## This will be used to remove any bad rows after examining them.\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "status_check_udf = fun.udf(bad_ping, BooleanType())\n",
    "\n",
    "webext_exp_dataset_check = webext_exp_dataset.withColumn(\"badPing\",\n",
    "    status_check_udf(webext_exp_dataset.e10sCohort, \n",
    "                     webext_exp_dataset.settings, \n",
    "                     webext_exp_dataset.addons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any bad pings, describe the problems and remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "webext_exp_dataset_bad = webext_exp_dataset_check.filter(\"badPing\")\\\n",
    "    .select(\"e10sCohort\", \"settings\", \"addons\")\\\n",
    "    .rdd\n",
    "\n",
    "has_bad = not webext_exp_dataset_bad.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issues:\n",
      "(u'multiBucket1', Row(e10s_enabled=False, only_webextension=True)): 281\n",
      "(u'webextensions-multiBucket4', Row(e10s_enabled=False, only_webextension=True)): 506\n",
      "(u'multiBucket1', Row(e10s_enabled=False, only_webextension=False)): 28973\n",
      "(u'webextensions-multiBucket4', Row(e10s_enabled=False, only_webextension=False)): 82\n",
      "(u'webextensions-multiBucket1', Row(e10s_enabled=True, only_webextension=False)): 2650\n",
      "(u'webextensions-multiBucket1', Row(e10s_enabled=False, only_webextension=True)): 496\n",
      "(u'multiBucket4', Row(e10s_enabled=False, only_webextension=False)): 29096\n",
      "(u'webextensions-multiBucket4', Row(e10s_enabled=True, only_webextension=False)): 2585\n",
      "(u'webextensions-multiBucket1', Row(e10s_enabled=False, only_webextension=False)): 72\n",
      "(u'multiBucket4', Row(e10s_enabled=False, only_webextension=True)): 274\n"
     ]
    }
   ],
   "source": [
    "if not has_bad:\n",
    "    print(\"No issues\")\n",
    "else:\n",
    "    check_counts = webext_exp_dataset_bad\\\n",
    "        .map(lambda r: (r.e10sCohort, e10s_status_check(r.settings, r.addons)))\\\n",
    "        .countByValue()\n",
    "    print(\"Issues:\")\n",
    "    for k, v in check_counts.iteritems():\n",
    "        print(\"{}: {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Removing these pings from the dataset. The dataset now contains 1363557 clients"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if has_bad:\n",
    "    webext_exp_dataset = webext_exp_dataset_check.filter(\"not badPing\").drop(\"badPing\")\n",
    "    print_with_markdown(\"\\nRemoving these pings from the dataset. The dataset now contains {} clients\".format(webext_exp_dataset.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_os(s):\n",
    "    sys_field = json.loads(s)\n",
    "    return sys_field.get(\"os\", {}).get(\"name\")\n",
    "\n",
    "parse_os = fun.udf(parse_os, st.StringType())\n",
    "webext_exp_dataset = webext_exp_dataset.withColumn('os_e10sCohort', \n",
    "                                                   fun.concat(parse_os('system'), fun.lit('_'), \"e10sCohort\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out clients with Windows XP. At this point, how many clients are left in each cohort by OS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Darwin_multiBucket1': 3659,\n",
       " u'Darwin_multiBucket4': 3570,\n",
       " u'Darwin_webextensions-multiBucket1': 224,\n",
       " u'Darwin_webextensions-multiBucket4': 204,\n",
       " u'Linux_multiBucket1': 102,\n",
       " u'Linux_multiBucket4': 114,\n",
       " u'Linux_webextensions-multiBucket1': 2,\n",
       " u'Linux_webextensions-multiBucket4': 3,\n",
       " u'Windows_NT_multiBucket1': 641238,\n",
       " u'Windows_NT_multiBucket4': 640478,\n",
       " u'Windows_NT_webextensions-multiBucket1': 36999,\n",
       " u'Windows_NT_webextensions-multiBucket4': 36687}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def notxp(p):\n",
    "    s_os = json.loads(p).get('os', {})\n",
    "    name = s_os.get('name')\n",
    "    version = s_os.get('version')\n",
    "    return name != \"Windows_NT\" or version != \"5.1\"\n",
    "\n",
    "notxp = fun.udf(notxp, st.BooleanType())\n",
    "webext_exp_dataset = webext_exp_dataset.filter(notxp('system'))\n",
    "\n",
    "cohort_sizes_row = webext_exp_dataset.groupBy(['os_e10sCohort']).count().collect()\n",
    "cohort_sizes = {}\n",
    "for i in cohort_sizes_row:\n",
    "    cohort_sizes[i.os_e10sCohort] = i['count']\n",
    "cohort_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sampling with the following proportions:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{u'Darwin_multiBucket1': 1,\n",
       " u'Darwin_multiBucket4': 1,\n",
       " u'Darwin_webextensions-multiBucket1': 1,\n",
       " u'Darwin_webextensions-multiBucket4': 1,\n",
       " u'Linux_multiBucket1': 1,\n",
       " u'Linux_multiBucket4': 1,\n",
       " u'Linux_webextensions-multiBucket1': 1,\n",
       " u'Linux_webextensions-multiBucket4': 1,\n",
       " u'Windows_NT_multiBucket1': 0.055,\n",
       " u'Windows_NT_multiBucket4': 0.055,\n",
       " u'Windows_NT_webextensions-multiBucket1': 1,\n",
       " u'Windows_NT_webextensions-multiBucket4': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sampling_props = {}\n",
    "\n",
    "for i in cohort_sizes:\n",
    "    if \"Linux\" in i or \"Darwin\" in i or \"webextension\" in i:\n",
    "        sampling_props[i] = 1\n",
    "    else:\n",
    "        # roughly matches webextension bucket size for windows\n",
    "        sampling_props[i] = .055\n",
    "print_with_markdown(\"Sampling with the following proportions:\\n\")\n",
    "sampling_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "We now have the the following number of clients per cohort: (approximately)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{u'Darwin_multiBucket1': 3659.0,\n",
       " u'Darwin_multiBucket4': 3570.0,\n",
       " u'Darwin_webextensions-multiBucket1': 224.0,\n",
       " u'Darwin_webextensions-multiBucket4': 204.0,\n",
       " u'Linux_multiBucket1': 102.0,\n",
       " u'Linux_multiBucket4': 114.0,\n",
       " u'Linux_webextensions-multiBucket1': 2.0,\n",
       " u'Linux_webextensions-multiBucket4': 3.0,\n",
       " u'Windows_NT_multiBucket1': 35268.090000000004,\n",
       " u'Windows_NT_multiBucket4': 35226.29,\n",
       " u'Windows_NT_webextensions-multiBucket1': 36999.0,\n",
       " u'Windows_NT_webextensions-multiBucket4': 36687.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webext_exp_dataset = webext_exp_dataset.sampleBy('os_e10sCohort', sampling_props)\n",
    "print_with_markdown('\\nWe now have the the following number of clients per cohort: (approximately)')\n",
    "\n",
    "e10s_addon_cohort_sizes = {}\n",
    "for cohort in sampling_props:\n",
    "    e10s_addon_cohort_sizes[cohort] = cohort_sizes[cohort] * sampling_props[cohort]\n",
    "\n",
    "e10s_addon_cohort_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get engagement metrics\n",
    "start, end = RANGE.strip('v').split('_')\n",
    "ms = sqlContext.read.option(\"mergeSchema\", True).parquet(\"s3://telemetry-parquet/main_summary/v4\")\\\n",
    " .filter(\"submission_date_s3 >= '{}'\".format(start))\\\n",
    " .filter(\"submission_date_s3 <= '{}'\".format(end)) \\\n",
    " .filter(\"app_name = 'Firefox'\")\\\n",
    " .filter(\"normalized_channel = 'beta'\")\\\n",
    " .filter(\"app_version = '54.0'\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms_tabs = ms.select(['client_id','sample_id', 'scalar_parent_browser_engagement_max_concurrent_tab_count']) \\\n",
    "            .groupBy(['client_id', 'sample_id']).mean('scalar_parent_browser_engagement_max_concurrent_tab_count')\\\n",
    "            .withColumnRenamed(\"avg(scalar_parent_browser_engagement_max_concurrent_tab_count)\", \"mean_max_tabs\")\\\n",
    "            .withColumnRenamed(\"client_id\", \"clientId\")\\\n",
    "            .withColumnRenamed(\"sample_id\", \"sampleId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_tabs_per_client = {}\n",
    "# for i in ms_tabs:\n",
    "#     max_tabs_per_client[i.client_id] = i.mean_max_tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined = webext_exp_dataset.join(ms_tabs, ['sampleId', 'clientId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_active_addon_info(addons_str):\n",
    "    \"\"\" Return a list of currently enabled add-ons in the form (GUID, name, version, isSystem). \"\"\"\n",
    "    addons = json.loads(addons_str)\n",
    "    addons = addons.get(\"activeAddons\", {})\n",
    "    if not addons:\n",
    "        return []\n",
    "    return [(guid, meta.get(\"name\"), meta.get(\"isSystem\"), meta.get(\"isWebExtension\"), \n",
    "             meta.get('version')) for guid, meta in addons.iteritems()]\n",
    "\n",
    "\n",
    "def get_top_addons(df, cohort_filter, n_top=100):\n",
    "    cohort_num, cohort_table = dataset_installed_addons(\n",
    "        df.filter(cohort_filter),\n",
    "        n_top=n_top)\n",
    "    print(\"There were {:,} distinct add-ons installed across the '{}' cohort.\"\\\n",
    "          .format(cohort_num, cohort_filter))\n",
    "\n",
    "    cohort_table[\"n_installs\"] = cohort_table[\"n_installs\"]\n",
    "    cohort_table[\"pct_installed\"] = cohort_table[\"pct_installed\"]\n",
    "    return cohort_table\n",
    "\n",
    "def dataset_installed_addons(data, n_top=100):\n",
    "    \"\"\" Extract add-on info from a subset of the main dataset, and generate a table of top add-ons\n",
    "        with installation counts.\n",
    "        \n",
    "        Returns a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    data_addons = data.select(\"addons\").rdd.map(lambda row: row[\"addons\"])\n",
    "    data_addons.cache()\n",
    "    n_in_data = data_addons.count()\n",
    "    \n",
    "    ##  Get counts by add-on ID/name/isSystem value.\n",
    "    addon_counts = data_addons.flatMap(get_active_addon_info)\\\n",
    "        .map(lambda a: (a, 1))\\\n",
    "        .reduceByKey(add)\\\n",
    "        .map(lambda ((guid, name, sys, we, version), n): (guid, (name, sys, we, version, n)))\n",
    "    \n",
    "    ## Summarize using the most common name and isSystem value.\n",
    "    top_vals = addon_counts.reduceByKey(lambda a, b: a if a[-1] > b[-1] else b)\\\n",
    "        .map(lambda (guid, (name, sys, we, version, n)): (guid, (name, sys, we, version)))\n",
    "    n_installs = addon_counts.mapValues(lambda (name, sys, we, version, n): n)\\\n",
    "        .reduceByKey(add)\n",
    "    addon_info = top_vals.join(n_installs)\\\n",
    "        .map(lambda (guid, ((name, sys, we, version), n)): {\n",
    "                \"guid\": guid,\n",
    "                \"name\": name,\n",
    "                \"is_system\": sys,\n",
    "                \"is_webextension\": we,\n",
    "                \"version\":version,\n",
    "                \"n_installs\": n,\n",
    "                \"pct_installed\": n / n_in_data * 100\n",
    "            })\\\n",
    "        .sortBy(lambda info: info[\"n_installs\"], ascending=False)\n",
    "    \n",
    "    addon_info_coll = addon_info.collect() if not n_top else addon_info.take(n_top)\n",
    "    addon_info_table = pd.DataFrame(addon_info_coll)\n",
    "    addon_info_table = addon_info_table[[\"guid\", \"name\", \"version\",\"is_system\", \n",
    "                                         \"is_webextension\", \"n_installs\", \"pct_installed\"]]\n",
    "    ## Number rows from 1.\n",
    "    addon_info_table.index += 1\n",
    "    n_addons = addon_info.count()\n",
    "    data_addons.unpersist()\n",
    "    return (n_addons, addon_info_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def row_2_ping(row):\n",
    "    # work around to get \"processes\" (child payloads) into payload \n",
    "    submission = json.loads(row.submission) if row.submission else {}\n",
    "    processes = submission.get(\"payload\", {}).get(\"processes\", {})\n",
    "    ping = {\n",
    "        \"payload\": {\n",
    "                    \"simpleMeasurements\": json.loads(row.simpleMeasurements) if row.simpleMeasurements else {},\n",
    "                    \"histograms\": json.loads(row.histograms) if row.histograms else {},\n",
    "                    \"keyedHistograms\": json.loads(row.keyedHistograms) if row.keyedHistograms else {},\n",
    "                    \"threadHangStats\": json.loads(row.threadHangStats) if row.threadHangStats else {},\n",
    "                    \"processes\": processes\n",
    "                    },\n",
    "       \"has_webextension\": True if row.e10sCohort.startswith(\"webextensions\") else False,\n",
    "       \"system\": json.loads(row.system),\n",
    "       \"cohort\": row.e10sCohort,\n",
    "       \"mean_max_tabs\": row.mean_max_tabs}\n",
    "    return ping\n",
    "\n",
    "subset = joined.rdd.map(row_2_ping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_gecko_activity(ping):\n",
    "    try:\n",
    "        uptime = ping[\"payload\"].get(\"simpleMeasurements\", {}).get(\"totalTime\", -1) / 60\n",
    "    except TypeError:\n",
    "        uptime = 0\n",
    "    if uptime <= 0:\n",
    "        return ping\n",
    "\n",
    "    def get_hangs_per_minute(threads, thread_name, uptime):\n",
    "        for thread in threads:\n",
    "            if thread[\"name\"] == thread_name:\n",
    "                activity = thread[\"activity\"][\"values\"]\n",
    "                if activity:\n",
    "                    histogram = pd.Series(activity.values(), index=map(int, activity.keys())).sort_index()\n",
    "                    # 255 is upper bound for 128-255ms bucket.\n",
    "                    return histogram[histogram.index >= 255].sum() / uptime\n",
    "        return None\n",
    "\n",
    "    threads = ping[\"payload\"].get(\"threadHangStats\", {})\n",
    "    ping[\"parent_hangs_per_minute\"] = get_hangs_per_minute(threads, \"Gecko\", uptime)\n",
    "\n",
    "    child_payloads = ping[\"payload\"].get(\"childPayloads\", [])\n",
    "    child_hangs_per_minute = []\n",
    "    for payload in child_payloads:\n",
    "        try:\n",
    "            child_uptime = payload.get(\"simpleMeasurements\", {}).get(\"totalTime\", -1) / 60\n",
    "        except TypeError:\n",
    "            return ping\n",
    "        if child_uptime <= 0:\n",
    "            continue\n",
    "        child_threads = payload.get(\"threadHangStats\", {})\n",
    "        child_hangs = get_hangs_per_minute(child_threads, \"Gecko_Child\", child_uptime)\n",
    "        if child_hangs:\n",
    "            child_hangs_per_minute.append(child_hangs)\n",
    "\n",
    "    if len(child_hangs_per_minute) > 0:\n",
    "        ping[\"child_hangs_per_minute\"] = sum(child_hangs_per_minute) / len(child_hangs_per_minute)\n",
    "\n",
    "    return ping\n",
    "\n",
    "subset = subset.map(add_gecko_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_histograms(pings, *histogram_names, **kwargs):\n",
    "    return compare_e10s_histograms(pings, e10s_addon_cohort_sizes, *histogram_names, **kwargs)\n",
    "    \n",
    "def compare_count_histograms(pings, *histogram_names, **kwargs):\n",
    "    return compare_e10s_count_histograms(pings, e10s_addon_cohort_sizes, *histogram_names, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_hist(ping):\n",
    "    \"\"\" Rename the histogram for e10s profiles. \"\"\"\n",
    "    hist = ping.get(\"payload\", {}).get(\"histograms\", {})\n",
    "    if \"FX_TAB_SWITCH_TOTAL_E10S_MS\" in hist and \"FX_TAB_SWITCH_TOTAL_MS\" not in hist:\n",
    "        hist[\"FX_TAB_SWITCH_TOTAL_MS\"] = hist[\"FX_TAB_SWITCH_TOTAL_E10S_MS\"]\n",
    "    return ping\n",
    "\n",
    "subset = subset.map(fix_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset = subset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = subset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t3, t4 = subset.filter(lambda x: x.get(\"mean_max_tabs\") < 4), subset.filter(lambda x: x.get(\"mean_max_tabs\") >= 4)\n",
    "\n",
    "n3, n4 = t3.count(), t4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_with_markdown(\"\"\"\n",
    "We can average the `max_concurrent_tab_count` field over all subsessions for a profile, giving\n",
    "us a profile's \"average max tabs open\" value. We can then split our sample into profiles that \n",
    "have an average of 3 or less max tabs open per subsession, or an average of 4+ max tabs open per subsession:\n",
    "\"\"\")\n",
    "\n",
    "print_with_markdown(\"3 or less: {}%<br>4 or more: {}%\".format(round(n3/N, 4) * 100, \n",
    "                                                              round(n4/N, 4) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_tab_groups(g1, g2, hist, l1=\"1-3 Tabs\", l2=\"4+ Tabs\", count=False):\n",
    "    ret = []\n",
    "    f = compare_count_histograms if count else compare_histograms \n",
    "    for tab_group in ((g1, l1), (g2, l2)):\n",
    "        data, label = tab_group\n",
    "        print_with_markdown(\"## \" + label)\n",
    "        r = f(data,  hist)\n",
    "        r['tabs'] = label\n",
    "        ret.append(r)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jank-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GC_MAX_PAUSE_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, 'payload/histograms/GC_MAX_PAUSE_MS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CYCLE_COLLECTOR_MAX_PAUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/CYCLE_COLLECTOR_MAX_PAUSE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT_EVENT_RESPONSE_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/INPUT_EVENT_RESPONSE_MS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageLoad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX_PAGE_LOAD_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/FX_PAGE_LOAD_MS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## MemoryUsage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY_TOTAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/MEMORY_TOTAL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY_VSIZE_MAX_CONTIGUOUS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/MEMORY_VSIZE_MAX_CONTIGUOUS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY_DISTRIBUTION_AMONG_CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/MEMORY_DISTRIBUTION_AMONG_CONTENT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TabSwitching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX_TAB_SWITCH_TOTAL_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/FX_TAB_SWITCH_TOTAL_MS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX_TAB_SWITCH_UPDATE_MS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/FX_TAB_SWITCH_UPDATE_MS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX_TAB_SWITCH_SPINNER_VISIBLE_MS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/FX_TAB_SWITCH_SPINNER_VISIBLE_MS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX_TAB_SWITCH_SPINNER_VISIBLE_LONG_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/FX_TAB_SWITCH_SPINNER_VISIBLE_LONG_MS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX_TAB_REMOTE_NAVIGATION_DELAY_MS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/FX_TAB_REMOTE_NAVIGATION_DELAY_MS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SlowScripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLOW_SCRIPT_NOTICE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/SLOW_SCRIPT_NOTICE_COUNT\", count=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLOW_SCRIPT_PAGE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/SLOW_SCRIPT_PAGE_COUNT\", count=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### SLOW_SCRIPT_NOTIFY_DELAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.extend(compare_tab_groups(t3, t4, \"payload/histograms/SLOW_SCRIPT_NOTIFY_DELAY\", count=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Wrangling summary data together for trending week over week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_summary_data(results):\n",
    "    '''\n",
    "    Unnests results to from one row for every os, probe, and cohort, \n",
    "    creating individual columns for each decile\n",
    "    '''\n",
    "    suffix_map = {\n",
    "        '_1' : 'webextensions-multiBucket1',\n",
    "        '_4': 'webextensions-multiBucket4',\n",
    "        '_1_std': 'multiBucket1', \n",
    "        '_4_std': 'multiBucket4'\n",
    "    }\n",
    "    ret = []\n",
    "    for result in results[1:]:\n",
    "        tabs = result.pop('tabs')\n",
    "        for os in result:\n",
    "            for probe in result[os]:\n",
    "                for suffix in suffix_map:\n",
    "                    curr = {'os': os, 'probe':probe, 'cohort':suffix_map[suffix], 'tabs': tabs}\n",
    "                    curr['n'] = result[os][probe][\"n\" + suffix]\n",
    "                    decile = result[os][probe][\"dec\" + suffix]\n",
    "                    p = 'pvalue_14_std' if 'std' in suffix else 'pvalue_14'\n",
    "                    curr['pvalue'] = result[os][probe][p]\n",
    "                        \n",
    "                    for i in range(10):\n",
    "                        curr['decile_{}'.format(i)] = decile[i]\n",
    "                    ret.append(curr)\n",
    "    return ret\n",
    "\n",
    "s = get_summary_data(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s =  pd.DataFrame(s)\n",
    "cols = ['probe', 'os', 'cohort', 'tabs', 'n', 'pvalue'] + ['decile_{}'.format(i) for i in range(10)]\n",
    "s = s.sort_values(['probe', 'os', 'cohort', 'tabs'])[cols]\n",
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.to_csv(\"./html/summary_multi_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concerns = s[(s.pvalue <= .05) & (s.n >= 1000)][['probe', 'os', 'cohort', 'tabs', 'n', 'pvalue']]\n",
    "for c in concerns.iterrows():\n",
    "    d = c[1]\n",
    "    if '1' in d.cohort:\n",
    "        comparison = d.cohort + '-4'\n",
    "        probe = \"<b>\" + d.probe + \"</b><br>\"\n",
    "        print_with_markdown(probe)\n",
    "        print_with_markdown(', '.join([d['os'], comparison , str(d.pvalue)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top Hang Stacks per Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stacks(subset):\n",
    "    def yield_ping_stacks(ping):\n",
    "        for thread in ping[\"payload\"][\"threadHangStats\"]:\n",
    "            if thread[\"name\"] != \"Gecko\":\n",
    "                continue\n",
    "            for hang in thread[\"hangs\"]:\n",
    "                if not hang[\"stack\"]:\n",
    "                    continue\n",
    "                values = hang[\"histogram\"][\"values\"]\n",
    "                histogram = pd.Series(values.values(), index=map(int, values.keys())).sort_index()\n",
    "                min_ms = 100\n",
    "                over_min_ms_count = histogram[histogram.index > min_ms].sum()\n",
    "                yield (tuple(hang[\"stack\"]), over_min_ms_count)\n",
    "    return subset.flatMap(yield_ping_stacks).reduceByKey(lambda a, b: a + b).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wm1 = get_stacks(subset.filter(lambda x: x['cohort'] == WEBEXTENSION_MULTI_1))\n",
    "wm4 = get_stacks(subset.filter(lambda x: x['cohort'] == WEBEXTENSION_MULTI_4))\n",
    "m1 = get_stacks(subset.filter(lambda x: x['cohort'] == MULTI_1))\n",
    "m4 = get_stacks(subset.filter(lambda x: x['cohort'] == MULTI_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_stacks_per_group(c1, c2, c3, c4, count, \n",
    "                             names=['Addons/E10s', 'Addons/NoE10s', 'NoAddons/E10s', 'NoAddons/NoE10s']):\n",
    "    '''\n",
    "    Takes the top <count> stacks per group in c1...4, ordered by\n",
    "    the ordering of the <names> list\n",
    "    \n",
    "    returns pandas DF indexed by the stack type\n",
    "    '''\n",
    "    c1_total_count = sum(c1.values())\n",
    "    c2_total_count = sum(c2.values())\n",
    "    c3_total_count = sum(c3.values())\n",
    "    c4_total_count = sum(c4.values())\n",
    "    result = []\n",
    "    index = []\n",
    "    for c, c1_stack_count in Counter(c1).most_common(count):\n",
    "        row = {}\n",
    "        c2_stack_count = c2.get(c, 0)\n",
    "        c3_stack_count = c3.get(c, 0)\n",
    "        c4_stack_count = c4.get(c, 0)\n",
    "        row = {names[0]: 100.0 * c1_stack_count / c1_total_count,\n",
    "               names[1]: 100.0 * c2_stack_count / c2_total_count,\n",
    "               names[2]: 100.0 * c3_stack_count / c3_total_count,\n",
    "               names[3]:100.0 * c4_stack_count / c4_total_count}\n",
    "        index.append(\"<br>\".join(reversed(c)))\n",
    "        result.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(result)[names]\n",
    "    df.index = index\n",
    "    return df.applymap(lambda x: '{:.2f}'.format(float(x)) + '%')\n",
    "\n",
    "def heat_map_df(df, color='red', css='background', null_color='white'):\n",
    "    '''\n",
    "    Colors in background of a percentage table\n",
    "    based on value.\n",
    "    \n",
    "    returns stylized dataframe (different from pd.Dataframe)\n",
    "    '''\n",
    "    color = Color(color)\n",
    "    dfmax = max(df.applymap(lambda x: float(x.split('%')[0])).max())\n",
    "    n_colors = int(math.ceil(dfmax))\n",
    "    gradient = list(reversed(list(color.range_to(Color(null_color),n_colors+1))))\n",
    "    \n",
    "    gradient_map = {}\n",
    "    for i in range(n_colors+1):\n",
    "        gradient_map[i] = gradient[i]\n",
    "    \n",
    "    css_style = lambda x: \"%s: %s\" % \\\n",
    "       (css, gradient_map[int(math.ceil(float(x.split('%')[0])))])\n",
    "    df1 = df.style.applymap(css_style)\n",
    "    return df1\n",
    "\n",
    "def heatmapify(df):\n",
    "    '''\n",
    "    Takes a pandas df and applies heatmap function to cells\n",
    "    with some cleaner css\n",
    "    \n",
    "    returns raw html\n",
    "    '''\n",
    "\n",
    "\n",
    "    css_header = '''\n",
    "    \n",
    "    <style  type=\"text/css\" >\n",
    "    \n",
    "    table {\n",
    "        font-family:Arial, Helvetica, sans-serif;\n",
    "        font-size:12px;\n",
    "        background:#eaebec;\n",
    "        margin:0 auto;\n",
    "        border:#ccc 1px solid;\n",
    "\n",
    "        -moz-border-radius:3px;\n",
    "        -webkit-border-radius:3px;\n",
    "        border-radius:3px;\n",
    "\n",
    "    }\n",
    "\n",
    "    table th {\n",
    "        padding:21px 25px 22px 25px;\n",
    "        border-top:1px solid #fafafa;\n",
    "        border-bottom:1px solid #e0e0e0;\n",
    "    }\n",
    "\n",
    "    table th:first-child {\n",
    "        text-align: left;\n",
    "        padding-left:20px;\n",
    "    }\n",
    "\n",
    "\n",
    "    table tr {\n",
    "        text-align: center;\n",
    "        padding-left:20px;\n",
    "    }\n",
    "\n",
    "    table td {\n",
    "        padding:18px;\n",
    "        border-top: 1px solid #ffffff;\n",
    "        border-bottom:1px solid #e0e0e0;\n",
    "        border-left: 1px solid #e0e0e0;\n",
    "\n",
    "    }\n",
    "    \n",
    "    tr:hover{\n",
    "        background-color: #CBCDCF;\n",
    "    }\n",
    "    '''\n",
    "    formatted_table = css_header + '\\n' + heat_map_df(df).render()\\\n",
    "                                          .split('<style  type=\"text/css\" >')[1]\n",
    "    return formatted_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wm1s = get_top_stacks_per_group(wm1, wm4, m1, m4, 25,\n",
    "        names = [WEBEXTENSION_MULTI_1, WEBEXTENSION_MULTI_4, MULTI_1, MULTI_4])\n",
    "wm4s  = get_top_stacks_per_group(wm4, wm1, m1, m4,25,\n",
    "        names = [WEBEXTENSION_MULTI_4, WEBEXTENSION_MULTI_1, MULTI_1, MULTI_4])\n",
    "m1s = get_top_stacks_per_group(m1, m4, wm1, wm4, 25,\n",
    "        names = [MULTI_1, MULTI_4, WEBEXTENSION_MULTI_1, WEBEXTENSION_MULTI_4])\n",
    "m4s = get_top_stacks_per_group(m4, m1, wm1, wm4,25,\n",
    "        names = [MULTI_4, MULTI_1, WEBEXTENSION_MULTI_1, WEBEXTENSION_MULTI_4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from colour import Color\n",
    "\n",
    "\n",
    "\n",
    "cohorts = (wm1s, wm4s, m1s, m4s)\n",
    "filenames = ('wm1.html', 'wm4.html', 'm1.html', 'm4.html')\n",
    "for i in range(4):\n",
    "    html = heatmapify(cohorts[i])\n",
    "    with open('./html/' + filenames[i], 'w') as f:\n",
    "        f.write(html)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
